{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgievw/ML/blob/main/Degree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpWDSDOL9dqC",
        "outputId": "91d515a5-0650-4be2-8b06-4334c6c3a267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Natasha\n",
            "  Downloading natasha-1.5.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.6.0\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.1-py3-none-any.whl (33 kB)\n",
            "Collecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting razdel>=0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from navec>=0.9.0->Natasha) (1.22.4)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->Natasha) (2.4.0)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=a7e40490e01002e66733c09d013c67a9ff7327fd1c116d7d0114c34d77f96acd\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26114 sha256=94dd08879976df3e2011ebdd23805c6e4059d1ba88c7db538e3e637cf2a60934\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/fa/1b/75d9a713279796785711bd0bad8334aaace560c0bd28830c8c\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, Natasha\n",
            "Successfully installed Natasha-1.5.0 dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 yargy-0.15.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "#Начало\n",
        "!pip install Natasha\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kvRoJwiG9wkk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    #NewsNERTagger,\n",
        "    #PER,\n",
        "    #NamesExtractor,\n",
        "    Doc\n",
        ")\n",
        "#Токенизатор?\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "#ner_tagger = NewsNERTagger(emb)\n",
        "#names_extractor = NamesExtractor(morph_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "915uYWAQVBJH"
      },
      "outputs": [],
      "source": [
        "class Span:\n",
        "  def __init__(self, tokens=list()):\n",
        "    self.tokens = tokens\n",
        "\n",
        "  def add_position_token(self, new_token, old_token = None, place = None):\n",
        "    if len(self.tokens) == 0:\n",
        "      self.tokens.append(new_token)\n",
        "      self.root = new_token\n",
        "    else:\n",
        "      self.tokens.insert(self.tokens.index(old_token)+place, new_token)\n",
        "    return\n",
        "\n",
        "  def add_token(self, token):\n",
        "    if len(self.tokens) == 0:\n",
        "      self.root = token\n",
        "    self.tokens.append(token)\n",
        "    return\n",
        "\n",
        "  def pop_first_token(self):\n",
        "    return self.tokens.pop(0)\n",
        "  \n",
        "  @property\n",
        "  def lemma(self):\n",
        "    for token in self.tokens:\n",
        "      token.lemmatize(morph_vocab)\n",
        "      token.text = token.lemma\n",
        "      if token is self.root:\n",
        "        break\n",
        "    return self.text #TODO\n",
        "  \n",
        "  @property\n",
        "  def text(self):\n",
        "    return ' '.join(token.text for token in self.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R3fa60qisCCS"
      },
      "outputs": [],
      "source": [
        "class Fact:\n",
        "  def __init__(self):\n",
        "    self.obj = Span([])\n",
        "    self.rel = Span([])\n",
        "    self.subj = Span([])\n",
        "  \n",
        "  def is_complete(self):\n",
        "    if len(self.obj.text) > 0 and len(self.subj.text) > 0 and len(self.rel.text) > 0:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def to_text(self):\n",
        "    if self.obj.tokens[0].rel == \"case\":\n",
        "      self.rel.add_token(self.obj.pop_first_token())\n",
        "    return self.subj.lemma.lower() + ',' + self.rel.text.lower() + ',' + self.obj.lemma.lower()\n",
        "\n",
        "  def to_tuple(self):\n",
        "    return (self.subj.lemma.lower(), self.rel.text.lower(), self.obj.lemma.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UkIWA4Btdy8P"
      },
      "outputs": [],
      "source": [
        "class Sentence:\n",
        "  def __init__(self, sent):\n",
        "    self.tokens = [0]\n",
        "    self.tokens.extend(sent.tokens)\n",
        "    self.syntax_graph = list()\n",
        "    self.construct_syntax_graph(sent)\n",
        "    self.fact = Fact()\n",
        "    #print(sent.text) #test\n",
        "    #print(self.syntax_graph) PRINT\n",
        "\n",
        "  def construct_syntax_graph(self, sent):\n",
        "    self.syntax_graph = [list() for i in range(len(sent.tokens)+1)]\n",
        "    for token in sent.tokens:\n",
        "      self.syntax_graph[int(token.head_id.split('_')[1])].append(int(token.id.split('_')[1]))\n",
        "    return self.syntax_graph\n",
        "\n",
        "  def test_root(self):\n",
        "    #print('1:', self.fact.to_text(), id(self.fact))\n",
        "    #self.fact = 0\n",
        "    self.fact = Fact()\n",
        "    #print('2:', self.fact.to_text(), id(self.fact))\n",
        "    if self.syntax_graph[0]:\n",
        "      root_index = self.syntax_graph[0][0]\n",
        "      self.find_relation(self.fact, root_index)\n",
        "      #print(self.tokens[fact[1]].text, self.tokens[fact[1]].rel) PRINT\n",
        "    else:\n",
        "      print(\"ERROR: No root token\")\n",
        "      return\n",
        "    #print(self.tokens[root_index].text, self.tokens[root_index].rel)\n",
        "    for next_token in self.syntax_graph[root_index]:\n",
        "      if self.tokens[next_token].rel[:5] == \"nsubj\":\n",
        "        self.find_subject(self.fact, next_token)\n",
        "      elif self.tokens[next_token].rel in  (\"obj\", \"obl\", 'xcomp'):\n",
        "        self.find_object(self.fact, next_token)\n",
        "      #print(self.tokens[next_token].text, self.tokens[next_token].rel) #PRINT\n",
        "\n",
        "    if self.fact.is_complete():\n",
        "      print(\"OK:\", self.fact.to_text())\n",
        "      print(self.fact)\n",
        "    else:\n",
        "      #print(\"ERROR:\", self.fact.to_text())\n",
        "      return None\n",
        "\n",
        "    return self.fact\n",
        "\n",
        "  def find_relation(self, fact, index):\n",
        "    fact.rel.add_token(self.tokens[index])\n",
        "    for next_token in self.syntax_graph[index]:\n",
        "      if self.tokens[next_token].rel == 'fixed':#'xcomp':\n",
        "        fact.rel.add_token(self.tokens[next_token])\n",
        "\n",
        "  def find_subject(self, fact, index):\n",
        "    fact.subj.add_token(self.tokens[index])\n",
        "    for next_token in self.syntax_graph[index]:\n",
        "      if self.tokens[next_token].rel in ('amod', 'case'):\n",
        "        fact.subj.add_position_token(self.tokens[next_token], self.tokens[index], 0)\n",
        "      elif self.tokens[next_token].rel == 'nmod':\n",
        "        self.find_subject(fact, next_token)\n",
        "    return 0\n",
        "\n",
        "  def find_object(self, fact, index):\n",
        "    fact.obj.add_token(self.tokens[index])\n",
        "    for next_token in self.syntax_graph[index]:\n",
        "      if self.tokens[next_token].rel in ('amod', 'case'):\n",
        "        fact.obj.add_position_token(self.tokens[next_token], self.tokens[index], 0)\n",
        "      elif self.tokens[next_token].rel == 'nmod':\n",
        "        self.find_object(fact, next_token)\n",
        "    return 0\n",
        "\n",
        "  def print_paths(self, path=\"\", cur_node=0):\n",
        "    if not self.syntax_graph[cur_node]:\n",
        "      print(path+\"#\")\n",
        "      return\n",
        "    for next_node in self.syntax_graph[cur_node]:\n",
        "      self.print_paths(path+self.tokens[next_node].text+'('+self.tokens[next_node].rel+')', next_node) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "40A_KCpyw9u2"
      },
      "outputs": [],
      "source": [
        "class FactExtractor:\n",
        "  def __init__(self):\n",
        "    self.facts = list()\n",
        "    self.context = list()\n",
        "\n",
        "  def extract(self, text=\"\"):\n",
        "    for dirpath, dirnames, filenames in os.walk(\"./Folder1\"):\n",
        "    # перебрать каталоги\n",
        "    #for dirname in dirnames:\n",
        "    #    print(\"Каталог:\", os.path.join(dirpath, dirname))\n",
        "    # перебрать файлы\n",
        "      for filename in filenames:\n",
        "          print(\"Файл:\", os.path.join(dirpath, filename))\n",
        "          if filename.split('.')[1]== \"txt\":\n",
        "            with open(os.path.join(dirpath, filename), 'r') as f:\n",
        "              text = f.read()\n",
        "              doc = Doc(text)\n",
        "              doc.segment(segmenter)\n",
        "              doc.tag_morph(morph_tagger)\n",
        "              doc.parse_syntax(syntax_parser)\n",
        "             #sent = doc.sents[0]\n",
        "              for sent in doc.sents:\n",
        "                s = Sentence(sent)\n",
        "               #print(s.__dict__)\n",
        "                f = s.test_root()\n",
        "                if f is not None:\n",
        "                  self.facts.append(f)\n",
        "          elif filename.split('.')[1]== \"pdf\":\n",
        "            reader = PdfReader(os.path.join(dirpath, filename))\n",
        "            number_of_pages = len(reader.pages)\n",
        "            for page in reader.pages:\n",
        "              text = page.extract_text()\n",
        "              doc = Doc(text)\n",
        "              doc.segment(segmenter)\n",
        "              doc.tag_morph(morph_tagger)\n",
        "              doc.parse_syntax(syntax_parser)\n",
        "             #sent = doc.sents[0]\n",
        "              for sent in doc.sents:\n",
        "                s = Sentence(sent)\n",
        "               #print(s.__dict__)\n",
        "                f = s.test_root()\n",
        "                if f is not None:\n",
        "                  self.facts.append(f)\n",
        "              #print(facts)\n",
        "                        #print(text)\n",
        "    return 0\n",
        "\n",
        "  def export_csv(self, name='loh.csv'):\n",
        "    f_out = open(name, 'w')\n",
        "    for fact in self.facts:\n",
        "      f_out.write(fact.to_text()+'\\n')\n",
        "    f_out.close()\n",
        "    return 0\n",
        "\n",
        "  def export_rdf(self, name='loh.csv'):\n",
        "    return 0\n",
        "\n",
        "  def export_tuples(self):\n",
        "    tuples = list()\n",
        "    for fact in self.facts:\n",
        "      tuples.append(fact.to_tuple())\n",
        "    return tuples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Ontology:\n",
        "  def __init__(self, facts):\n",
        "    d_obj = dict()\n",
        "    d_rel = dict()\n",
        "    for t in facts:\n",
        "      if t[0] in d_obj:\n",
        "        d_obj[t[0]] = d_obj[t[0]] + 1\n",
        "      else: \n",
        "        d_obj[t[0]] = 1\n",
        "      if t[2] in d_obj:\n",
        "        d_obj[t[2]] = d_obj[t[2]] + 1\n",
        "      else: \n",
        "        d_obj[t[2]] = 1\n",
        "      if t[1] in d_rel:\n",
        "        d_rel[t[1]] = d_rel[t[1]] + 1\n",
        "      else: \n",
        "        d_rel[t[1]] = 1\n",
        "    self.correct_obj = [obj for obj in d_obj if d_obj[obj] > 1]\n",
        "    self.correct_rel = [rel for rel in d_rel if d_rel[rel] > 1]\n",
        "    self.triplets = set([fact for fact in facts if fact[0] in self.correct_obj and fact[2] in self.correct_obj])\n",
        "    #correct_facts = [fact for fact in l if fact[0] in correct_obj or fact[2] in correct_obj]\n",
        "    #correct_facts = [fact for fact in l if fact[1] in correct_rel]\n",
        "  \n",
        "  def export_RDF(self):\n",
        "    f = open('test.rdf', 'w')\n",
        "    f.write(header)\n",
        "    for triplet in self.triplets:\n",
        "      f.write(rel_temp.format('_'.join(triplet[1].split()), '_'.join(triplet[0].split()), '_'.join(triplet[2].split())))\n",
        "    for obj in self.correct_obj:\n",
        "      f.write(obj_temp.format('_'.join(obj.split())))\n",
        "    f.write(end)\n",
        "    f.close()\n",
        "    return"
      ],
      "metadata": {
        "id": "BtVlPKFQGMuI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = \\\n",
        "'''<?xml version=\"1.0\"?>\n",
        "<rdf:RDF xmlns=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont/\"\n",
        "     xml:base=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont/\"\n",
        "     xmlns:owl=\"http://www.w3.org/2002/07/owl#\"\n",
        "     xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
        "     xmlns:xml=\"http://www.w3.org/XML/1998/namespace\"\n",
        "     xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n",
        "     xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\">\n",
        "    <owl:Ontology rdf:about=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont\">\n",
        "        <rdfs:comment>Тестовая попытка</rdfs:comment>\n",
        "    </owl:Ontology>  \n",
        "'''\n",
        "end = \"</rdf:RDF>\"\n",
        "\n",
        "obj_temp = \\\n",
        "'''\n",
        "<owl:Class rdf:about=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont#{}\"/>\n",
        "'''\n",
        "\n",
        "rel_temp = \\\n",
        "'''\n",
        "    <owl:ObjectProperty rdf:about=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont#{}\">\n",
        "        <rdfs:domain rdf:resource=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont#{}\"/>\n",
        "        <rdfs:range rdf:resource=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont#{}\"/>\n",
        "    </owl:ObjectProperty>\n",
        "'''"
      ],
      "metadata": {
        "id": "5Tu4rUBO8s-n"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " w = 'дли на ху я'"
      ],
      "metadata": {
        "id": "rH9SC9feER45"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'_'.join(w.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FamNeQX_EV5J",
        "outputId": "e683cbff-7589-4793-ce11-9e06188287bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'дли_на_ху_я'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsD1pVCeRe1f"
      },
      "outputs": [],
      "source": [
        "test = FactExtractor()\n",
        "test.extract()\n",
        "#test.export_csv('страховая.csv')\n",
        "l = test.export_tuples()\n",
        "onto = Ontology(l)\n",
        "onto.export_RDF()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onto.export_RDF()"
      ],
      "metadata": {
        "id": "LS1Cc96xgOx4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmuNUwENmMWa"
      },
      "outputs": [],
      "source": [
        "#f = open(\"1.txt\")\n",
        "#text = f.read()\n",
        "facts = list()\n",
        "doc = Doc(text)\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "doc.parse_syntax(syntax_parser)\n",
        "#sent = doc.sents[0]\n",
        "for sent in doc.sents:\n",
        "  s = Sentence(sent)\n",
        "  #print(s.__dict__)\n",
        "  f = s.test_root()\n",
        "  if f is not None:\n",
        "    facts.append(f)\n",
        "  print(facts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCkZFLefHZi6"
      },
      "outputs": [],
      "source": [
        "for fact in facts:\n",
        "  if len(fact.to_text()) < 100:\n",
        "    print(fact.to_text())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "dtWVKeNt_FBY",
        "outputId": "0dccd5f7-9a60-4e86-cc7c-e29821482e98"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n<?xml version=\"1.0\"?>\\n<rdf:RDF xmlns=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont/\"\\n     xml:base=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont/\"\\n     xmlns:owl=\"http://www.w3.org/2002/07/owl#\"\\n     xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\\n     xmlns:xml=\"http://www.w3.org/XML/1998/namespace\"\\n     xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\\n     xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\">\\n    <owl:Ontology rdf:about=\"http://www.semanticweb.org/georgxxl/ontologies/2023/3/testont\">\\n        <rdfs:comment>Тестовая попытка</rdfs:comment>\\n    </owl:Ontology>  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cringe.format(\"жорикс\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wU7zB6F1_Kb7",
        "outputId": "8efc9972-3125-41db-d7e6-dbc5b7229f19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ты лох, жорикс'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxYk3bfc93oJ"
      },
      "outputs": [],
      "source": [
        "#text = \"Машинное обучение (англ. machine learning, ML) — это класс методов искусственного интеллекта, характерной чертой которых является не прямое решение задачи, а обучение за счёт применения решений множества сходных задач. Для построения таких методов используются средства математической статистики, численных методов, математического анализа, методов оптимизации, теории вероятностей, теории графов, различные техники работы с данными в цифровой форме.\"\n",
        "#text = \"Машинное обучение используется для решения задач, которые помогают бизнесу и упрощают жизнь\"\n",
        "#text = \"Язык используется для кунилингуса, который является лаской, которая используется для получения наслаждения.\"\n",
        "#text = \"Лодейников собирается придумывать план\"\n",
        "#text = \"Лодейников считает, что нужно придумать план\"\n",
        "#text = \"Лодейников думает, что язык - часть тела человека\"\n",
        "#text = \"Российская противотанковая мина нажимного действия\"\n",
        "#text = \"Котя и Пёся - спортсмены и отличники\"\n",
        "#text = \"Бил, Гей, используется для поцелуев\"\n",
        "#text = \"Язык - часть тела, которая используются для поцелуев\"\n",
        "#text = \"Когда петухи поют, в городе наступает ночь\"\n",
        "#text = \"Непервый пирог оказался вкусным, когда мы его съели\"\n",
        "#text = \"Я не буду делать ни эту математику, ни этот русский\"\n",
        "#text = \"Ах, как красиво тут!\"\n",
        "text = \"Противотанковая мина нажимного действия используется для уничтожения техники противника.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQMceplU-BLe",
        "outputId": "ed424af9-fec8-4b3d-c609-01659ad65296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DocSent(start=192, stop=413, text='Было\\tсделано\\tнемало\\tнаучных\\tоткрытий,\\tинжене..., tokens=[...])\n"
          ]
        }
      ],
      "source": [
        "#f = open(\"1.txt\")\n",
        "#text = f.read()\n",
        "doc = Doc(text)\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "doc.parse_syntax(syntax_parser)\n",
        "sent = doc.sents[1]\n",
        "print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzd6OZ8R-uqE",
        "outputId": "802993a4-e98d-43b3-8989-da20e6284d11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  ┌► Было           aux:pass\n",
            "┌───────────────┌─└─ сделано        \n",
            "│             ┌─└──► немало         nsubj:pass\n",
            "│             │   ┌► научных        amod\n",
            "│   ┌─┌─────┌─└──►└─ открытий       obl\n",
            "│   │ │     │   ┌──► ,              punct\n",
            "│   │ │     │   │ ┌► инженерных     amod\n",
            "│   │ │     └──►└─└─ изобретений    conj\n",
            "│   │ │       ┌────► ,              punct\n",
            "│   │ │       │   ┌► на             case\n",
            "│ ┌►│ │       │ ┌─└─ основе         obl\n",
            "│ │ │ │       │ └──► которых        nmod\n",
            "│ │ │ │       └─┌─── создавались    acl:relcl\n",
            "│ │ │ │       │ │ ┌► целые          amod\n",
            "│ │ │ │       │ └►└─ отрасли        nsubj:pass\n",
            "│ │ │ │ ┌─────└─└──► промышленности nmod\n",
            "│ │ │ │ │     │   ┌► (              punct\n",
            "│ │ │ │ │ ┌─┌─└──►└─ машиностроение parataxis\n",
            "│ │ │ │ │ │ │     ┌► ,              punct\n",
            "│ │ │ │ │ │ └────►└─ энергетика     conj\n",
            "│ │ │ │ │ │     ┌──► ,              punct\n",
            "│ │ │ │ │ │     │ ┌► цифровые       amod\n",
            "│ │ │ └►│ │     └─└─ технологии     conj\n",
            "│ │ │   │ │       ┌► и              cc\n",
            "│ │ └──►│ │ ┌─┌─┌─└─ т              conj\n",
            "│ │     │ │ │ │ └──► .              punct\n",
            "│ │     │ │ │ └────► д              fixed\n",
            "│ │     │ │ └──────► .              punct\n",
            "│ │     │ └────────► )              punct\n",
            "│ │     │     ┌────► ,              punct\n",
            "│ │     │     │ ┌──► которые        nsubj\n",
            "│ │     │     │ │ ┌► значительно    advmod\n",
            "│ └─────└────►└─└─└─ облегчили      acl:relcl\n",
            "│               └►┌─ жизнь          obj\n",
            "│                 └► людей          nmod\n",
            "└──────────────────► .              punct\n"
          ]
        }
      ],
      "source": [
        "sent.syntax.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gxrO8Z9DEIk",
        "outputId": "05a02bc2-2edd-4755-f885-11e1d4461a40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SyntaxToken(\n",
              "     id='2_1',\n",
              "     text='Было',\n",
              "     head_id='2_2',\n",
              "     rel='aux:pass'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_2',\n",
              "     text='сделано',\n",
              "     head_id='2_0',\n",
              "     rel='root'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_3',\n",
              "     text='немало',\n",
              "     head_id='2_2',\n",
              "     rel='nsubj:pass'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_4',\n",
              "     text='научных',\n",
              "     head_id='2_5',\n",
              "     rel='amod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_5',\n",
              "     text='открытий',\n",
              "     head_id='2_3',\n",
              "     rel='obl'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_6',\n",
              "     text=',',\n",
              "     head_id='2_8',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_7',\n",
              "     text='инженерных',\n",
              "     head_id='2_8',\n",
              "     rel='amod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_8',\n",
              "     text='изобретений',\n",
              "     head_id='2_5',\n",
              "     rel='conj'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_9',\n",
              "     text=',',\n",
              "     head_id='2_13',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_10',\n",
              "     text='на',\n",
              "     head_id='2_11',\n",
              "     rel='case'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_11',\n",
              "     text='основе',\n",
              "     head_id='2_33',\n",
              "     rel='obl'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_12',\n",
              "     text='которых',\n",
              "     head_id='2_11',\n",
              "     rel='nmod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_13',\n",
              "     text='создавались',\n",
              "     head_id='2_16',\n",
              "     rel='acl:relcl'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_14',\n",
              "     text='целые',\n",
              "     head_id='2_15',\n",
              "     rel='amod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_15',\n",
              "     text='отрасли',\n",
              "     head_id='2_13',\n",
              "     rel='nsubj:pass'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_16',\n",
              "     text='промышленности',\n",
              "     head_id='2_15',\n",
              "     rel='nmod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_17',\n",
              "     text='(',\n",
              "     head_id='2_18',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_18',\n",
              "     text='машиностроение',\n",
              "     head_id='2_16',\n",
              "     rel='parataxis'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_19',\n",
              "     text=',',\n",
              "     head_id='2_20',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_20',\n",
              "     text='энергетика',\n",
              "     head_id='2_18',\n",
              "     rel='conj'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_21',\n",
              "     text=',',\n",
              "     head_id='2_23',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_22',\n",
              "     text='цифровые',\n",
              "     head_id='2_23',\n",
              "     rel='amod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_23',\n",
              "     text='технологии',\n",
              "     head_id='2_5',\n",
              "     rel='conj'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_24',\n",
              "     text='и',\n",
              "     head_id='2_25',\n",
              "     rel='cc'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_25',\n",
              "     text='т',\n",
              "     head_id='2_5',\n",
              "     rel='conj'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_26',\n",
              "     text='.',\n",
              "     head_id='2_25',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_27',\n",
              "     text='д',\n",
              "     head_id='2_25',\n",
              "     rel='fixed'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_28',\n",
              "     text='.',\n",
              "     head_id='2_25',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_29',\n",
              "     text=')',\n",
              "     head_id='2_18',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_30',\n",
              "     text=',',\n",
              "     head_id='2_33',\n",
              "     rel='punct'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_31',\n",
              "     text='которые',\n",
              "     head_id='2_33',\n",
              "     rel='nsubj'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_32',\n",
              "     text='значительно',\n",
              "     head_id='2_33',\n",
              "     rel='advmod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_33',\n",
              "     text='облегчили',\n",
              "     head_id='2_16',\n",
              "     rel='acl:relcl'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_34',\n",
              "     text='жизнь',\n",
              "     head_id='2_33',\n",
              "     rel='obj'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_35',\n",
              "     text='людей',\n",
              "     head_id='2_34',\n",
              "     rel='nmod'\n",
              " ),\n",
              " SyntaxToken(\n",
              "     id='2_36',\n",
              "     text='.',\n",
              "     head_id='2_2',\n",
              "     rel='punct'\n",
              " )]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent.syntax.tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpcLO8JcGtSY"
      },
      "source": [
        "--------------------------------------------------------------ПОМОЙКА------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K29yRVp6NADa"
      },
      "outputs": [],
      "source": [
        "#Попытка извлечение бич-фактов 1\n",
        "import os\n",
        "\n",
        "def FactExtract(file_path: str = \"\") -> list:\n",
        "  terms = list()\n",
        "  np_chunking = ['NOUN', 'ADJ', 'ADP']\n",
        "  for dirpath, dirnames, filenames in os.walk(os.path.join(\".\", file_path)):\n",
        "    for filename in filenames:\n",
        "        #print(\"Файл:\", os.path.join(dirpath, filename))\n",
        "        with open(os.path.join(dirpath, filename), 'r') as f:\n",
        "          text = f.read()\n",
        "          doc = nlp(text)\n",
        "          cur_term = list()\n",
        "          cur_poses = set()\n",
        "          is_cur_term = False\n",
        "          obj1 = \"\"\n",
        "          relation = \"\"\n",
        "          for token in doc:\n",
        "            if str(token.pos_) == \"VERB\":\n",
        "              relation = token.text\n",
        "            elif str(token.pos_) in np_chunking:\n",
        "              cur_term.append(token.text)\n",
        "              cur_poses.add(token.pos_)\n",
        "              is_cur_term = True\n",
        "              print(token.text, token.lemma_, token.pos_, token.is_stop)\n",
        "            \n",
        "            else:\n",
        "              if is_cur_term:\n",
        "                print('<-')\n",
        "                is_cur_term = False\n",
        "                if \"NOUN\" in cur_poses:\n",
        "                  if obj1 and relation:\n",
        "                    terms.append((obj1, relation, ' '.join(cur_term)))\n",
        "                    obj1 = \"\"\n",
        "                  else:\n",
        "                    obj1 = ' '.join(cur_term)\n",
        "                    relation = \"\"\n",
        "                  #terms.append(' '.join(cur_term))\n",
        "                cur_term = list()\n",
        "                cur_poses = set()\n",
        "            #print(token.text, token.lemma_, token.pos_, token.is_stop)\n",
        "            #print(token.pos_)\n",
        "          if \"NOUN\" in cur_poses:\n",
        "              terms.append(' '.join(cur_term))\n",
        "  return terms\n",
        "\n",
        "facts = FactExtract(\"Folder1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldm91JBfOqHq"
      },
      "outputs": [],
      "source": [
        "facts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3TgRu4Z8fno"
      },
      "outputs": [],
      "source": [
        "for dirpath, dirnames, filenames in os.walk(\"./Folder1\"):\n",
        "    # перебрать каталоги\n",
        "    #for dirname in dirnames:\n",
        "    #    print(\"Каталог:\", os.path.join(dirpath, dirname))\n",
        "    # перебрать файлы\n",
        "    for filename in filenames:\n",
        "        print(\"Файл:\", os.path.join(dirpath, filename))\n",
        "        with open(os.path.join(dirpath, filename), 'r') as f:\n",
        "          text = f.read()\n",
        "          print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyW-ovkV7n2P",
        "outputId": "6afc41c0-aff3-42b6-8158-a44220640755"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.ipynb_checkpoints', '2.txt', '3.txt', '1.txt']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(os.path.join(\".\", 'Folder1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEkonZzb3fTU"
      },
      "outputs": [],
      "source": [
        "for t1, t2, t3 in os.walk(\".\"):\n",
        "  print(t1, t2, t3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dyv7nc9g376M"
      },
      "outputs": [],
      "source": [
        "f = open(\"./Folder1/1.txt\")\n",
        "print(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nghs1iOtZq4v"
      },
      "outputs": [],
      "source": [
        "#Тестируем модуль построения OWL\n",
        "test_relations1 = [\n",
        "    (\"Мина нажимного действия\", \"Является\", \"Мина\"),\n",
        "    (\"Война\", \"Это\", \"Конфликт\"),\n",
        "    (\"Оружие\", \"Используется для\", \"Уничтожение живой силы\")\n",
        "]\n",
        "\n",
        "test_relations2 = [\n",
        "    (\"Мина нажимного действия\", \"Is\", \"Мина\"),\n",
        "    (\"Война\", \"Is\", \"Конфликт\"),\n",
        "    (\"Оружие\", \"IsUsedFor\", \"Уничтожение живой силы\")\n",
        "]\n",
        "\n",
        "construct_OWL(test_relations1, \"test1.txt\")\n",
        "construct_OWL(test_relations2, \"test2.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wMX7kKO1Wqh"
      },
      "outputs": [],
      "source": [
        "#Попытка извлечение терминов 1\n",
        "import os\n",
        "\n",
        "def TermExtract(file_path: str = \"\") -> list:\n",
        "  terms = list()\n",
        "  np_chunking = ['NOUN', 'ADJ', 'ADP']\n",
        "  for dirpath, dirnames, filenames in os.walk(os.path.join(\".\", file_path)):\n",
        "    for filename in filenames:\n",
        "        #print(\"Файл:\", os.path.join(dirpath, filename))\n",
        "        with open(os.path.join(dirpath, filename), 'r') as f:\n",
        "          text = f.read()\n",
        "          doc = nlp(text)\n",
        "          cur_term = list()\n",
        "          cur_poses = set()\n",
        "          is_cur_term = False\n",
        "          for token in doc:\n",
        "            if str(token.pos_) in np_chunking:\n",
        "              cur_term.append(token.text)\n",
        "              cur_poses.add(token.pos_)\n",
        "              is_cur_term = True\n",
        "              print(token.text, token.lemma_, token.pos_, token.is_stop)\n",
        "            else:\n",
        "              if is_cur_term:\n",
        "                print('<-')\n",
        "                is_cur_term = False\n",
        "                if \"NOUN\" in cur_poses:\n",
        "                  terms.append(' '.join(cur_term))\n",
        "                cur_term = list()\n",
        "                cur_poses = set()\n",
        "            #print(token.text, token.lemma_, token.pos_, token.is_stop)\n",
        "            #print(token.pos_)\n",
        "          if \"NOUN\" in cur_poses:\n",
        "              terms.append(' '.join(cur_term))\n",
        "  return terms\n",
        "\n",
        "terms = TermExtract(\"Folder1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHLROUluY-BZ"
      },
      "outputs": [],
      "source": [
        "def construct_OWL(relations: list, file_name: str = \"output.txt\"):\n",
        "  f = open(file_name, 'w')\n",
        "  for relation in relations:\n",
        "    f.write(str(relation)+'\\n')\n",
        "  f.close()\n",
        "  return"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6ON/qW8MsBLRrmRi9yw9W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}